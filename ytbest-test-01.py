import streamlit as st
from googleapiclient.discovery import build
from googletrans import Translator
import re
from collections import Counter
from datetime import datetime, timedelta
import statistics
import random

# --- [ì„¤ì •] API í‚¤ ê´€ë¦¬ ---
API_KEYS = ["AIzaSyAZeKYF34snfhN1UY3EZAHMmv_IcVvKhAc", "AIzaSyBNMVMMfFI5b7GNEXjoEuOLdX_zQ8XjsCc"]
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"

st.set_page_config(page_title="Global Trend Intelligence", layout="wide")

if 'key_index' not in st.session_state:
    st.session_state.key_index = 0

# CSS ë””ìì¸
st.markdown("""
<style>
    .video-card { 
        background-color: #ffffff; 
        padding: 15px; 
        border-radius: 12px; 
        border: 1px solid #e0e0e0; 
        margin-bottom: 20px; 
        box-shadow: 0 2px 8px rgba(0,0,0,0.05); 
        height: 100%;
    }
    .thumb-link img { transition: transform 0.2s; border-radius: 8px; width: 100%; aspect-ratio: 16/9; object-fit: cover; }
    .thumb-link img:hover { transform: scale(1.02); }
    
    .v-title { font-size: 0.95rem; font-weight: 700; color: #111; line-height: 1.35; max-height: 2.7em; overflow: hidden; margin: 10px 0 5px 0; }
    .v-meta { font-size: 0.8rem; color: #555; margin-bottom: 8px; line-height: 1.4; border-bottom: 1px dashed #eee; padding-bottom: 8px; }
    
    .v-status { display: inline-block; padding: 3px 6px; border-radius: 4px; font-size: 0.7rem; font-weight: bold; margin-bottom: 5px; }
    .status-10d { background-color: #ffebee; color: #c62828; }
    .status-1m { background-color: #e3f2fd; color: #1565c0; }
    .status-steady { background-color: #f5f5f5; color: #616161; }
    
    .v-insight-box { background-color: #f8f9fa; padding: 10px; border-radius: 8px; font-size: 0.8rem; border-left: 3px solid #1a73e8; }
    
    .report-container { background-color: #1e293b; color: #f1f5f9; padding: 30px; border-radius: 15px; margin-top: 40px; box-shadow: 0 10px 25px rgba(0,0,0,0.2); }
    .report-title { color: #38bdf8; font-size: 1.5rem; font-weight: bold; margin-bottom: 20px; border-bottom: 1px solid #475569; padding-bottom: 10px; }
    .report-section { margin-bottom: 15px; }
    .report-label { color: #94a3b8; font-size: 0.85rem; font-weight: bold; text-transform: uppercase; letter-spacing: 1px; }
    .report-content { font-size: 1rem; line-height: 1.7; margin-top: 5px; }
    .highlight { color: #facc15; font-weight: bold; }
    .stat-val { color: #1a73e8; font-weight: 800; }
</style>
""", unsafe_allow_html=True)

# ê´‘ê³  í•¨ìˆ˜
def show_ad_banner(position):
    ad_data = {
        "sidebar": {"img": "https://via.placeholder.com/300x250.png?text=US+Trend+Ads", "link": "#"},
        "top": {"img": "https://via.placeholder.com/468x60.png?text=Premium+Analytics", "link": "#"},
        "bottom": {"img": "https://via.placeholder.com/728x90.png?text=Marketing+Solutions", "link": "#"}
    }
    data = ad_data.get(position)
    if data:
        st.markdown(f"""<div style="text-align:center; margin:10px 0;"><a href="{data['link']}" target="_blank"><img src="{data['img']}" style="width:100%; border-radius:8px;"></a></div>""", unsafe_allow_html=True)

# ìƒë‹¨ ë ˆì´ì•„ì›ƒ
col_title, col_ad = st.columns([3, 1])
with col_title: st.title("ğŸ“¡ ì‹¤ì‹œê°„ ê¸€ë¡œë²Œ íŠ¸ë Œë“œ ì¸í…”ë¦¬ì „ìŠ¤")
with col_ad: show_ad_banner("top")

translator = Translator()

def get_youtube_client():
    return build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEYS[st.session_state.key_index])

def parse_duration(duration):
    minutes = re.search(r'(\d+)M', duration)
    seconds = re.search(r'(\d+)S', duration)
    total = 0
    if minutes: total += int(minutes.group(1)) * 60
    if seconds: total += int(seconds.group(1))
    return total

def calculate_viral_point(views, likes, comments):
    if views == 0: return 0
    engagement = (likes / views * 10) + (comments / views * 50)
    return int((views * 0.001) * (1 + engagement))

def is_japanese(text):
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', text))

def is_strictly_non_us(title, channel):
    """ë¯¸êµ­ íƒ€ê²Ÿ ì‹œ ì¸ë„/ë™ë‚¨ì•„ ì½˜í…ì¸  ê°•ë ¥ ì°¨ë‹¨"""
    # 1. ë¬¸ìì—´(Script) ê°ì§€
    scripts = [re.compile(r'[\u0900-\u097F]+'), re.compile(r'[\u0E00-\u0E7F]+'), re.compile(r'[\u0600-\u06FF]+'), re.compile(r'[\u0B80-\u0BFF]+')]
    combined = title + " " + channel
    if any(s.search(combined) for s in scripts): return True
    
    # 2. í‚¤ì›Œë“œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ (ì¸ë„/ë™ë‚¨ì•„ ëŒ€í˜• ì±„ë„ ë° ì§€ëª…)
    blacklist = [
        'india', 'hindi', 'bollywood', 't-series', 'zeemusic', 'set india', 'sony pal', 'colors tv', 'sab tv', 'star plus', 
        'telugu', 'tamil', 'punjabi', 'thai', 'vietnam', 'philippines', 'indonesia', 'malay', 'v-pop', 't-pop', 
        'gmmgrammy', 'abs-cbn', 'workpoint', 'bhakti', 'bhojpuri', 'desimelodies', 'speed records'
    ]
    return any(k in combined.lower() for k in blacklist)

def generate_expert_report(region_name, video_type, results, keywords):
    if not results: return "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    avg_views = statistics.mean([v['view_raw'] for v in results])
    avg_viral = statistics.mean([v['v_point'] for v in results])
    top_keywords = [k for k, c in Counter(keywords).most_common(3)]
    
    context = ""
    if "ì¼ë³¸" in region_name and "Shorts" in video_type:
        context = "ì¼ë³¸ ìˆí¼ ì‹œì¥ì€ 'ë²„ì¸„ì–¼ ìœ íŠœë²„', 'ì• ë‹ˆë©”ì´ì…˜ 2ì°¨ ì°½ì‘'ì´ ê°•ì„¸ì´ë©°, ì–¸ì–´ì  ìœ í¬ê°€ í¬í•¨ëœ ë°ˆ(Meme)ì´ ë¹ ë¥´ê²Œ í™•ì‚°ë©ë‹ˆë‹¤."
    elif "ë¯¸êµ­" in region_name:
        context = "ë¯¸êµ­ ì‹œì¥ì€ 'ë‰´ìš•/LA ê¸°ë°˜ì˜ ì–´ë°˜ ë¼ì´í”„ìŠ¤íƒ€ì¼'ê³¼ 'ê¸€ë¡œë²Œ ì±Œë¦°ì§€'ê°€ í•µì‹¬ì´ë©°, ì¸ë„ ë“± íƒ€ ë¬¸í™”ê¶Œ ì½˜í…ì¸ ë¥¼ ë°°ì œí•œ ìˆœìˆ˜ ë¶ë¯¸ íŠ¸ë Œë“œê°€ ì£¼ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤."
    elif "í•œêµ­" in region_name:
        context = "í•œêµ­ ì‹œì¥ì€ 'ê³µê°ëŒ€ í˜•ì„±'ê³¼ 'ë¹ ë¥¸ ì •ë³´ ì „ë‹¬'ì´ í•µì‹¬ì´ë©°, ëŒ“ê¸€ ì»¤ë®¤ë‹ˆí‹°ê°€ íŠ¸ë Œë“œ ì§€ì†ì„±ì„ ê²°ì •í•©ë‹ˆë‹¤."

    return f"""
    <div class="report-container">
        <div class="report-title">ğŸ“Š 2026 {region_name} ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ</div>
        <div class="report-section">
            <div class="report-label">Target Market Analysis</div>
            <div class="report-content">
                í˜„ì¬ <b>{region_name}</b>ì˜ {video_type} ì‹œì¥ì€ í‰ê·  ì¡°íšŒìˆ˜ <span class="highlight">{int(avg_views):,}íšŒ</span>, í‰ê·  Viral Point <span class="highlight">{int(avg_viral):,}ì </span>ì„ ê¸°ë¡ ì¤‘ì…ë‹ˆë‹¤. 
                {context}
            </div>
        </div>
        <div class="report-section">
            <div class="report-label">Content DNA</div>
            <div class="report-content">
                ìƒìœ„ ë­í¬ëœ ì½˜í…ì¸ ë“¤ì˜ í•µì‹¬ DNAëŠ” <b>'{", ".join(top_keywords)}'</b>ì…ë‹ˆë‹¤. 
                íŠ¹íˆ ë¯¸êµ­ ì‹œì¥ ë¶„ì„ ì‹œ, <b>'Non-US Content 10% Cap'</b> ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ì¸ë„ ë° ë™ë‚¨ì•„ì‹œì•„ ìœ ì…ì„ ì°¨ë‹¨í•˜ê³  ìˆœìˆ˜ ë¶ë¯¸ê¶Œ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.
            </div>
        </div>
        <hr style="border-color: #475569;">
        <div style="font-size: 0.8rem; color: #94a3b8;">
            * ë³¸ ë¦¬í¬íŠ¸ëŠ” ì‹¤ì‹œê°„ ìˆ˜ì§‘ëœ {len(results)}ê±´ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
        </div>
    </div>
    """

def fetch_videos(topic_text, v_type, r_info, v_count):
    youtube = get_youtube_client()
    is_shorts = "Shorts" in v_type
    is_popular_mode = not topic_text.strip()
    
    collected_items = []
    next_page_token = None
    # ì¸ë„ ì˜ìƒì´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í‰ì†Œë³´ë‹¤ ë” ë§ì€ ë°ì´í„°ë¥¼ ìŠ¤ìº”í•¨ (ìµœëŒ€ 300ê°œ)
    max_scan_pages = 6 if r_info['code'] == 'US' else 4
    
    # [í•µì‹¬] ë¯¸êµ­ íƒ€ê²Ÿ ì‹œ ê²€ìƒ‰ì–´ì— 'ì œì™¸ í‚¤ì›Œë“œ(-)'ë¥¼ ê°•ë ¥í•˜ê²Œ ì ìš©
    us_filter = "-india -hindi -bollywood -tamil -telugu" if r_info['code'] == 'US' else ""
    
    for _ in range(max_scan_pages):
        try:
            if not is_popular_mode:
                try: translated_q = translator.translate(topic_text, dest=r_info['lang']).text
                except: translated_q = topic_text
                
                # ê²€ìƒ‰ ì¿¼ë¦¬ì— ì œì™¸ í•„í„° ì¶”ê°€
                final_q = f"{translated_q} {us_filter} {'#shorts' if is_shorts else ''}"
                
                request = youtube.search().list(
                    part="snippet", q=final_q, 
                    type="video", videoDuration="short" if is_shorts else "any", 
                    regionCode=r_info['code'], relevanceLanguage=r_info['lang'], 
                    order="viewCount", maxResults=50, pageToken=next_page_token
                )
            else:
                if is_shorts:
                    country_kw = {"KR": "ì‡¼ì¸ ", "US": "Shorts", "JP": "ã‚·ãƒ§ãƒ¼ãƒˆ"}
                    # ìˆì¸  ê³µë€ ê²€ìƒ‰ ì‹œì—ë„ ì œì™¸ í•„í„° ë° ì£¼ìš” ë„ì‹œ í‚¤ì›Œë“œ íŒíŠ¸ ì ìš© ê°€ëŠ¥
                    base_q = f"#shorts {country_kw.get(r_info['code'], '')}"
                    final_q = f"{base_q} {us_filter}"
                    
                    request = youtube.search().list(
                        part="snippet", q=final_q, 
                        type="video", videoDuration="short", 
                        regionCode=r_info['code'], relevanceLanguage=r_info['lang'], 
                        order="viewCount", maxResults=50, pageToken=next_page_token
                    )
                else:
                    request = youtube.videos().list(
                        part="snippet,statistics", chart="mostPopular", 
                        regionCode=r_info['code'], maxResults=50, pageToken=next_page_token
                    )
            
            response = request.execute()
            collected_items.extend(response.get('items', []))
            next_page_token = response.get('nextPageToken')
            
            if not next_page_token: break
            # ëª©í‘œ ìˆ˜ëŸ‰ì˜ 3ë°°ìˆ˜ë¥¼ í™•ë³´í•˜ë©´ ìŠ¤ìº” ì¤‘ë‹¨ (í•„í„°ë§ ê³ ë ¤)
            if len(collected_items) >= v_count * 3: break
            
        except Exception as e:
            if "quotaExceeded" in str(e): raise e
            break

    video_ids = [item['id']['videoId'] if 'videoId' in item['id'] else item['id'] for item in collected_items]
    if not video_ids: return [], 0, [], ""

    # ìƒì„¸ ë°ì´í„° ì¡°íšŒ (50ê°œì”©)
    all_stats_items = []
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        try:
            stats_resp = youtube.videos().list(
                part="snippet,statistics,contentDetails", id=",".join(chunk)
            ).execute()
            all_stats_items.extend(stats_resp.get('items', []))
        except: continue

    results = []
    trend_keywords = []
    now = datetime.now()
    
    # [í•µì‹¬] ë¹„ìœ¨ ì œí•œ ì¹´ìš´í„°
    non_us_count = 0
    max_non_us_allowed = int(v_count * 0.1) # 10% ì´í•˜ ì œí•œ

    for item in all_stats_items:
        title = item['snippet']['title']
        channel = item['snippet']['channelTitle']
        duration_sec = parse_duration(item['contentDetails']['duration'])
        
        # 1ë…„ ì§€ë‚œ ì˜ìƒ ì œì™¸
        days_diff = (now - datetime.strptime(item['snippet']['publishedAt'], "%Y-%m-%dT%H:%M:%SZ")).days
        if days_diff > 365: continue
        
        if not is_shorts and duration_sec < 120: continue 
        if is_shorts and duration_sec > 120: continue
        
        # [êµ­ê°€ë³„ í•„í„°]
        if r_info['code'] == 'JP' and not is_japanese(title + channel): continue
        
        # [ë¯¸êµ­ íƒ€ê²Ÿ í•„í„°] ì¸ë„/ë™ë‚¨ì•„ 10% ìº¡(Cap) ì ìš©
        if r_info['code'] == 'US':
            if is_strictly_non_us(title, channel):
                if non_us_count >= max_non_us_allowed: continue # 10% ë„˜ìœ¼ë©´ ë²„ë¦¼
                non_us_count += 1

        views = int(item['statistics'].get('viewCount', 0))
        likes = int(item['statistics'].get('likeCount', 0)) if 'likeCount' in item['statistics'] else 0
        comments = int(item['statistics'].get('commentCount', 0)) if 'commentCount' in item['statistics'] else 0
        
        if days_diff > 30 and (views < 500000 or (likes+comments)/views < 0.02): continue

        v_point = calculate_viral_point(views, likes, comments)
        
        if days_diff <= 10: tier, status = 1, "ğŸ”¥ 10ì¼ë‚´ ì´ˆì‹ ì„±"
        elif days_diff <= 30: tier, status = 2, "ğŸ“… ì›”ê°„ íŠ¸ë Œë“œ"
        else: tier, status = 3, "ğŸ”„ ìŠ¤í…Œë””ì…€ëŸ¬"

        words = re.sub(r'[^\w\s]', '', title).split()
        trend_keywords.extend([w for w in words if len(w) > 1])

        results.append({
            'title': title, 'thumbnail': item['snippet']['thumbnails']['high']['url'],
            'url': f"https://www.youtube.com/shorts/{item['id']}" if is_shorts else f"https://www.youtube.com/watch?v={item['id']}",
            'channel': channel, 'view_count': views, 'date': item['snippet']['publishedAt'][:10],
            'v_point': v_point, 'status': status, 'tier': tier, 'view_raw': views
        })

    results.sort(key=lambda x: (x['tier'], -x['v_point']))
    final_list = results[:v_count]
    
    display_name = f"{r_info['code']} ì‹œì¥"
    if r_info['code'] == 'KR': display_name = "í•œêµ­ ğŸ‡°ğŸ‡·"
    elif r_info['code'] == 'US': display_name = "ë¯¸êµ­ ğŸ‡ºğŸ‡¸"
    elif r_info['code'] == 'JP': display_name = "ì¼ë³¸ ğŸ‡¯ğŸ‡µ"

    report_html = generate_expert_report(display_name, "Shorts" if is_shorts else "Long-form", final_list, trend_keywords)
    
    accuracy = (len(final_list)/v_count)*100 if v_count > 0 else 0
    return final_list, min(accuracy, 100.0), report_html

# --- ì‚¬ì´ë“œë°” ---
st.sidebar.header("ğŸ“Š ë¶„ì„ íŒŒë¼ë¯¸í„°")
region_map = {"í•œêµ­ ğŸ‡°ğŸ‡·": {"code": "KR", "lang": "ko"}, "ë¯¸êµ­ ğŸ‡ºğŸ‡¸": {"code": "US", "lang": "en"}, "ì¼ë³¸ ğŸ‡¯ğŸ‡µ": {"code": "JP", "lang": "ja"} }
region_name = st.sidebar.selectbox("ğŸ“ íƒ€ê²Ÿ ì‹œì¥", list(region_map.keys()))
sel_region = region_map[region_name]
video_type = st.sidebar.radio("ğŸ“± ì½˜í…ì¸  í¬ë§·", ["ë¡±í¼ (2ë¶„ ì´ìƒ)", "ìˆí¼ (Shorts)"])
count = st.sidebar.slider("ğŸ”¢ ë¶„ì„ ìƒ˜í”Œ", 1, 30, 8)
topic = st.sidebar.text_input("ğŸ” í‚¤ì›Œë“œ/ì£¼ì œ", placeholder="ê³µë€: ì‹¤ì‹œê°„ ì¸ê¸° ìˆ˜ì§‘")
search_clicked = st.sidebar.button("ğŸš€ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘", use_container_width=True)

st.sidebar.markdown("---")
with st.sidebar:
    show_ad_banner("sidebar")

# --- ê²°ê³¼ ì¶œë ¥ ---
if search_clicked or not topic:
    with st.spinner('ì´ˆì •ë°€ í•„í„°ë§ ë° ì‹œë‹ˆì–´ ë¦¬í¬íŠ¸ ì‘ì„± ì¤‘...'):
        try:
            final_results, accuracy, report_html = fetch_videos(topic, video_type, sel_region, count)
            st.subheader(f"ğŸ“ {region_name} {video_type} ë¶„ì„ ê²°ê³¼")
            
            if not final_results: st.warning("ì¡°ê±´ì— ë§ëŠ” íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ í™•ë³´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                cols = st.columns(4)
                for idx, video in enumerate(final_results):
                    with cols[idx % 4]:
                        s_class = "status-10d" if video['tier'] == 1 else ("status-1m" if video['tier'] == 2 else "status-steady")
                        st.markdown(f"""
                        <div class="video-card">
                            <a href="{video['url']}" target="_blank" class="thumb-link"><img src="{video['thumbnail']}"></a>
                            <div style="margin-top:10px;"><span class="v-status {s_class}">{video['status']}</span></div>
                            <div class="v-title">{video['title']}</div>
                            <div class="v-meta">
                                <b>{video['channel']}</b><br>
                                ì¡°íšŒìˆ˜: {video['view_count']:,}íšŒ<br>
                                ê³µê°œì¼: {video['date']}
                            </div>
                            <div class="v-insight-box">
                                ğŸŒ <b>Viral Point:</b> <span class="stat-val">{video['v_point']:,}</span>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
                
                st.markdown(report_html, unsafe_allow_html=True)
                
                col_empty, col_ad_bottom = st.columns([3, 1])
                with col_ad_bottom: show_ad_banner("bottom")

        except Exception as e:
            if "quotaExceeded" in str(e):
                if st.session_state.key_index < len(API_KEYS) - 1:
                    st.session_state.key_index += 1
                    st.toast("ğŸ”„ 1ë²ˆ í‚¤ ì†Œì§„! ìë™ í‚¤ ì „í™˜ ì¤‘...")
                    time.sleep(1)
                    st.rerun()
                else: st.error("ğŸš¨ ëª¨ë“  í• ë‹¹ëŸ‰ ì†Œì§„.")
            else: st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
```

### ğŸ“ ë¯¸êµ­ ì‹œì¥ í•„í„°ë§ ê°•í™” í¬ì¸íŠ¸
1.  **API ì›ì²œ ì°¨ë‹¨ (`us_filter`):** ê²€ìƒ‰ ì¿¼ë¦¬ ìì²´ì— `-india -hindi -bollywood` ë“± ë§ˆì´ë„ˆìŠ¤ ê²€ìƒ‰ì–´ë¥¼ ì¶”ê°€í•˜ì—¬, ìœ íŠœë¸Œê°€ ì• ì´ˆì— ì¸ë„ ì˜ìƒì„ ëœ ë³´ë‚´ì£¼ë„ë¡ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
2.  **ì—„ê²©í•œ 10% ì œí•œ (`max_non_us_target`):** ìˆ˜ì§‘ëœ ì˜ìƒ ì¤‘ì—ì„œë„ ì±„ë„ëª…ì´ë‚˜ ì œëª©ì— ì¸ë„/ë™ë‚¨ì•„ í”ì ì´ ìˆëŠ” ê²½ìš°, ì „ì²´ ë¦¬ìŠ¤íŠ¸ì˜ **10%ë¥¼ ë„˜ì§€ ëª»í•˜ë„ë¡ ì¹´ìš´íŒ…í•˜ì—¬ ê°•ì œ ìŠ¤í‚µ**í•©ë‹ˆë‹¤.
3.  **ë°ì´í„° í’€ í™•ëŒ€:** í•„í„°ë§ìœ¼ë¡œ ì¸í•´ ì˜ìƒì´ ë¶€ì¡±í•´ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ ë¯¸êµ­ íƒ€ê²Ÿ ì‹œ **ë°ì´í„° ìŠ¤ìº” ë²”ìœ„ë¥¼ 6í˜ì´ì§€(ìµœëŒ€ 300ê°œ)**ê¹Œì§€ ëŠ˜ë ¸ìŠµë‹ˆë‹¤.

ì´ì œ ë¯¸êµ­ ìˆí¼ì„ ê²€ìƒ‰í•˜ë©´ ì¸ë„ ë…¸ë˜ë‚˜ ë“œë¼ë§ˆ ì§¤ë°© ëŒ€ì‹ , **ì§„ì§œ ë¯¸êµ­ì˜ ë°ˆê³¼ ì±Œë¦°ì§€ ì˜ìƒ**ë“¤ì´ ì£¼ë¥¼ ì´ë£° ê²ƒì…ë‹ˆë‹¤. :)
68.8s
Model
ThinkingThoughts
Expand to view model thoughts

chevron_right
